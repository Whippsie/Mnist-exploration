{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 1\n",
    "\n",
    "## Initialisations\n",
    "### Appel des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : 1 if x > 250 else 0\n",
    "normalize = np.vectorize(f)\n",
    "mnist_train = normalize(np.loadtxt(\"mnist_train.csv\", delimiter=\",\", skiprows=1))\n",
    "\n",
    "mnist_test = normalize(np.loadtxt(\"mnist_test.csv\", delimiter=\",\", skiprows=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.14000000e+02 -6.61390948e+00 -1.62802840e+01 ... -7.09799957e+00\n",
      "  -2.02923118e+00 -3.15076570e+00]\n",
      " [ 1.32000000e+02  1.51991062e+00 -4.49430158e+01 ... -6.02863826e+00\n",
      "   1.65296472e+01  1.41162529e+00]\n",
      " [ 4.60000000e+01 -7.94559430e+00 -2.53291427e+01 ...  2.40202316e+00\n",
      "  -7.91897467e+00  2.37525272e+01]\n",
      " ...\n",
      " [ 1.24000000e+02  9.51190366e-01 -5.68239454e+01 ...  9.63703024e+00\n",
      "   9.82688116e-01  1.83143294e+00]\n",
      " [ 7.00000000e+01  3.19341540e+00 -1.73243318e+01 ...  4.71515550e+00\n",
      "  -6.13635909e+00 -3.04796333e+00]\n",
      " [ 9.20000000e+01 -4.19542887e+00 -4.07596663e+01 ...  1.84023781e+00\n",
      "  -9.77086555e-01 -7.74095237e-02]]\n"
     ]
    }
   ],
   "source": [
    "def dct(x):\n",
    "    N = x.shape[1]\n",
    "    \n",
    "    cos_mat = np.cos( np.pi/N * np.outer(np.arange(N) + 1.0/2.0, np.arange(N)))\n",
    "    \n",
    "    return 2 * np.dot(x, cos_mat)\n",
    "\n",
    "def dctI(x):\n",
    "    N = x.shape[1]\n",
    "    cos_mat = np.cos(np.pi/N * np.outer(np.arange(1, N), np.arange(N) + 1.0/2.0))\n",
    "    return  np.abs((x[:, 0]/2 + np.dot(x[:,1:], cos_mat))/N)\n",
    "\n",
    "mnist_train_dct = dct(mnist_train[:1000, 1:])\n",
    "#print(mnist_train[1,1:])\n",
    "print(mnist_train_dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_Ln(x1, x2, n=2):\n",
    "    return np.sum((x2 - x1)**n, axis=1)**(1.0/n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_mean:\n",
    "    def __init__(self, k, random_seed=0, distance=distance_Ln):\n",
    "        self._k = k\n",
    "        self._similarity = distance\n",
    "        self._random_seed = random_seed\n",
    "        \n",
    "        \n",
    "    def _get_closest_centroid(self, datas):\n",
    "        distances = np.zeros((len(datas), self._k))\n",
    "        i = 0\n",
    "        for centroid in self._centroids:\n",
    "            distances[:,i] = self._similarity(datas, centroid)\n",
    "            \n",
    "            #print(distances[:,i])\n",
    "            i += 1\n",
    "        \n",
    "        return np.argmin(distances, axis=1)\n",
    "    \n",
    "    def _initialize(self, datas):\n",
    "        \n",
    "        dim = datas.shape[1]\n",
    "        self._centroids = np.fill((self._k, dim), -1)\n",
    "        self._centroids[0] = train_set[np.random.randint(n)]\n",
    "        distances = np.zeros((len(datas), self._k))\n",
    "        ncentroid = 0\n",
    "        #for i in range(1, self._k):\n",
    "            #for j in range(ncentroid):\n",
    "                \n",
    "        \n",
    "    def train(self, train_set):\n",
    "        np.random.seed(self._random_seed)\n",
    "        n = len(train_set)\n",
    "        dim = train_set.shape[1]\n",
    "        self._centroids = np.zeros((self._k, dim))\n",
    "        self._centroids[0] = train_set[np.random.randint(n)]\n",
    "        \n",
    "        for i in range(1, self._k):\n",
    "            self._centroids[i] = train_set[np.random.randint(n)]\n",
    "         \n",
    "\n",
    "        groups = self._get_closest_centroid(train_set)\n",
    "        new_centroids = np.zeros((self._k, dim))\n",
    "        \n",
    "        while True:\n",
    "        \n",
    "            for i in range(self._k):\n",
    "                cent_group = train_set[groups == i]\n",
    "                if len(cent_group) > 0:\n",
    "                    new_centroids[i] = np.average(cent_group, axis=0)\n",
    "                else :\n",
    "                    new_centroids[i] = self._centroids[i]\n",
    "            \n",
    "            if np.array_equal(new_centroids,self._centroids):\n",
    "                #print(self._centroids)\n",
    "                return self\n",
    "            \n",
    "            self._centroids = new_centroids\n",
    "            \n",
    "            groups = self._get_closest_centroid(train_set)\n",
    "        \n",
    "        \n",
    "    def predict(self, test_set):\n",
    "        return self._get_closest_centroid(test_set)\n",
    "          \n",
    "\n",
    "k = 10\n",
    "random_state = 255\n",
    "#km = K_mean(k, random_seed=random_state).train(mnist_train_dct)\n",
    "km = KMeans(10, random_state=random_state).fit(mnist_train_dct)\n",
    "#print(km._centroids[1])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def printImage(datas):\n",
    "    #datas = [[int(datas[i + 28*j] * 255) for i in range(28)] for j in range(28)]\n",
    "    #print(datas)\n",
    "    datas = [min(255, int(255 * d)) for d in datas]\n",
    "    colors = bytes(datas)\n",
    "    \n",
    "    im = Image.frombytes(\"L\", (28,28), colors)\n",
    "    im.show()\n",
    "    \n",
    "\n",
    "for cent in km.cluster_centers_ :   \n",
    "    \n",
    "    inv_cent = dctI(cent.reshape(1,len(cent)))\n",
    "    #print(inv_cent)\n",
    "    #printImage(inv_cent.reshape(inv_cent.shape[1]))\n",
    "#printImage(mnist_train_dct[0,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03813499847033897"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = km.predict(mnist_train_dct[:1000])\n",
    "metrics.silhouette_score(mnist_train_dct[:1000], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0009470541656144587\n"
     ]
    }
   ],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=10).fit(mnist_train_dct)\n",
    "args = np.argwhere(clustering.labels_ == 0)\n",
    "args = args.reshape((len(args)))\n",
    "#print(args)\n",
    "average = np.average(mnist_train_dct[args], axis=0)\n",
    "#printImage(dctI(average.reshape(1, average.shape[0])).reshape(average.shape[0], 1))\n",
    "predictions = clustering.labels_\n",
    "print(metrics.silhouette_score(mnist_train_dct[:1000], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 784), (60000,))\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] n_neighbors=4, weights=uniform ..................................\n",
      "[CV] ................... n_neighbors=4, weights=uniform, total=16.7min\n",
      "[CV] n_neighbors=4, weights=uniform ..................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 49.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... n_neighbors=4, weights=uniform, total=16.6min\n",
      "[CV] n_neighbors=4, weights=uniform ..................................\n",
      "[CV] ................. n_neighbors=4, weights=uniform, total=1798.0min\n",
      "[CV] n_neighbors=4, weights=distance .................................\n",
      "[CV] ................ n_neighbors=4, weights=distance, total=1336.2min\n",
      "[CV] n_neighbors=4, weights=distance .................................\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mnist = pd.read_csv(\"mnist_train.csv\").values\n",
    "mnist_x, mnist_y = mnist[:,1:], mnist[:,0]\n",
    "print(mnist_x.shape, mnist_y.shape)\n",
    "\n",
    "# Implementing OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "mnist_y = ohe.fit_transform(mnist_y.reshape(-1,1))\n",
    "\n",
    "# Reshaping the image in a matrix (28x28 images) for visualization purposes only\n",
    "nfigs = 5\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(nfigs **2):\n",
    "    ax = fig.add_subplot(nfigs, nfigs, i+1)\n",
    "    ax.imshow(mnist_x[i].reshape(28,28))\n",
    "    \n",
    "# Using cross validation for <optimal> values of hyper-parameters\n",
    "# Note to self: Maybe I should remove this part, and evaluate with k manually\n",
    "grid_params = {\"n_neighbors\": (4,21), \"weights\":[\"uniform\",\"distance\"]}\n",
    "knn= KNeighborsClassifier()\n",
    "grid_search = GridSearchCV(knn,grid_params,verbose=2, n_jobs=1)\n",
    "grid_search.fit(mnist_x, mnist_y)\n",
    "\n",
    "# Let`s predict our test group now\n",
    "testing = pd.read_csv(mnist_train.csv)\n",
    "preds = grid_search.predict(testing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
